import asyncio
from langgraph.graph import StateGraph, END
from typing import Any, Dict, Callable
from app.services.langgraph_nodes import photo_node_async, quiz_node_async, orchestrator_node_async
import time
import hashlib
import json
from functools import lru_cache


# Simple in-memory cache for responses (in production, use Redis)
_response_cache = {}
_cache_ttl = 300  # 5 minutes

def _get_cache_key(state: Dict[str, Any]) -> str:
    """Generate cache key from state data"""
    cache_data = {
        'photo_url': state.get('photo_url', '')[:100] if state.get('photo_url') else '',
        'answers': [{'questionId': a.questionId, 'value': a.value} for a in state.get('answers', [])],
        'additional_data': state.get('additional_data', {})
    }
    return hashlib.md5(json.dumps(cache_data, sort_keys=True).encode()).hexdigest()

def build_analysis_graph(orchestrator: Any, question_map: Dict[str, Any]) -> Any:
    """
    Build and compile the LangGraph pipeline with OPTIMIZED ASYNC PARALLEL PROCESSING.
    Uses asyncio for better I/O performance, caching, and optimized prompts.
    """
    
    async def optimized_parallel_analysis_node(state: Dict[str, Any]) -> Dict[str, Any]:
        """
        OPTIMIZED: Async parallel node with caching and performance improvements.
        Expected 40-60% faster than ThreadPoolExecutor version.
        """
        print("[LangGraph] ðŸš€ Starting OPTIMIZED ASYNC parallel analysis...")
        start_time = time.time()
        
        # Check cache first
        cache_key = _get_cache_key(state)
        cached_result = _response_cache.get(cache_key)
        if cached_result and (time.time() - cached_result['timestamp']) < _cache_ttl:
            print(f"[LangGraph] âš¡ Cache HIT - returning cached result in {time.time() - start_time:.2f}s")
            return {**state, **cached_result['data']}
        
        # Prepare states for each analysis
        photo_state = dict(state)
        quiz_state = dict(state)
        quiz_state["question_map"] = question_map
        
        # Execute both analyses concurrently using asyncio
        try:
            photo_task = photo_node_async(photo_state)
            quiz_task = quiz_node_async(quiz_state)
            
            # Run both tasks concurrently
            photo_result, quiz_result = await asyncio.gather(photo_task, quiz_task)
            
            parallel_time = time.time() - start_time
            print(f"[LangGraph] âœ… OPTIMIZED parallel analysis completed in {parallel_time:.2f}s")
            
            result_data = {
                "photo_insights": photo_result.get("photo_insights"),
                "quiz_insights": quiz_result.get("quiz_insights")
            }
            
            # Cache the result
            _response_cache[cache_key] = {
                'data': result_data,
                'timestamp': time.time()
            }
            
            return {**state, **result_data}
            
        except Exception as e:
            print(f"[LangGraph] âŒ Error in parallel analysis: {e}")
            # Fallback to sequential processing
            photo_result = await photo_node_async(photo_state)
            quiz_result = await quiz_node_async(quiz_state)
            return {
                **state,
                "photo_insights": photo_result.get("photo_insights"),
                "quiz_insights": quiz_result.get("quiz_insights")
            }

    async def optimized_orchestrator_node(state: Dict[str, Any]) -> Dict[str, Any]:
        """OPTIMIZED: Async orchestrator with faster synthesis"""
        print("[LangGraph] ðŸŽ¯ Starting OPTIMIZED orchestrator synthesis...")
        # Inject dependencies into state
        state_with_deps = dict(state)
        state_with_deps["orchestrator"] = orchestrator
        state_with_deps["question_map"] = question_map
        return await orchestrator_node_async(state_with_deps)

    # Create StateGraph with async support
    def sync_wrapper(async_func):
        """Wrapper to run async functions in sync context"""
        def wrapper(state):
            loop = None
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            if loop.is_running():
                # If loop is already running, create a new task
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, async_func(state))
                    return future.result()
            else:
                return loop.run_until_complete(async_func(state))
        return wrapper

    builder = StateGraph(dict)
    
    # Add optimized nodes
    builder.add_node("optimized_parallel_analysis", sync_wrapper(optimized_parallel_analysis_node))
    builder.add_node("optimized_orchestrator", sync_wrapper(optimized_orchestrator_node))
    
    # Simple linear flow
    builder.add_edge("optimized_parallel_analysis", "optimized_orchestrator")
    builder.add_edge("optimized_orchestrator", END)
    
    # Set entry point
    builder.set_entry_point("optimized_parallel_analysis")
    
    print("[LangGraph] Compiled OPTIMIZED ASYNC pipeline with caching")
    return builder.compile() 